{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.7.6-final"
    },
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Calabria_labIntro",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hstorm/ML_course_Calabria2020/blob/master/lab/Calabria_labIntro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k10_r9GL8YlT",
        "colab_type": "text"
      },
      "source": [
        "### Intro notebook for the course: \"Machine Learning for Applied Economics and Policy\"\n",
        "\n",
        "#### Instructors\n",
        "- Kathy Baylis\n",
        "- Giovanni Cerulli\n",
        "- Gianluigi Greco\n",
        "- Thomas Heckelei\n",
        "- Hugo Storm\n",
        "\n",
        "#### Description\n",

        "This notebook is intended to get you familiar with some of the most common data science / ML libaries typically used in python. \n",
        "In this notebook you will 1) load data, 2) prepare the data for running your models, 3) run a simple logistic regression, 4) run a very simiple neural network, and 5) compare the results of the two models. You will learn in the course that a logistic regression is actually a special case of a very simple neural network! So if you have run a logistic regression you have actually worked with NN... \n",
 
        "\n",
        "Work Steps\n",
        "\n",
        "1. (If you are reading this in Github and haven't yet opened it in colab,) Open this notebook in google colab (https://colab.research.google.com/) using the link provided above. To run the notebook you need to have a google account. \n",
        "\n",
        "2. Execute all code cells below (Runtime/Run all) and try to understand what is going on.\n",
        "\n",
 
        "3. Two important python libraries for working with data in python are numpy and pandas \n",
        "    There are plenty of tutorials online to get you a first idea of how they work. Two examples are provided here. For taking the course you do not have to be an expert in using those libraries but having a first basic understanding of the functionality will certainly help you to follow the examples. \n",
        "    \n",
        "- Numpy: https://www.datacamp.com/community/tutorials/python-numpy-tutorial\n",
        "\n",
        "- Pandas: https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html#min   \n",
        "\n",
        "4. (Optional) Play around with the notebook and make some changes (no worries you can not break it...). Here are some ideas what you can try to achieve:\n",
        "\n",
        "- In the data set there are many more variables. Figure out how they are named and add a couple more variables to two models. Run the models and see how this changes the quality of the model prediction (in terms of R²). \n",
        "\n",
        "-  If you want to go a step further... Create some new variables by adding interaction terms or square/cube terms. See if this increases model performance (R²).\n",
        "\n",
 
        "- Are you up for the challenge (before even starting with the course)? The sklearn libary implements a large number of ML models. We will cover the most important ones in this course. In this notebook you have already seen how to use the logistic regression or a neural network in sklearn. Try to adjust the code to run an additional model, for example a random forest (will be covered on day 2 in the course). There are plenty of tutorials online (for example https://www.datacamp.com/community/tutorials/random-forests-classifier-python). Hint: there is basically only one line of code that you need to change in order to run an random forest with sklearn instead of a logistic regression. "

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9uNLCh58YlU",
        "colab_type": "text"
      },
      "source": [
        "#### Load relevant libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_wnzeZx8YlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import scipy.stats as stat\n",
        "from scipy.stats import norm\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01gwbHWy8Ylc",
        "colab_type": "text"
      },
      "source": [
        "#### Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rQhcbrS8Ylh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1cd1873d-f512-467c-df9b-002aef192911"
      },
      "source": [
        "# Download data\n",
        "!wget http://www.ilr.uni-bonn.de/agpo/courses/ml/brazil_all_data_v2.csv"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-27 05:31:42--  http://www.ilr.uni-bonn.de/agpo/courses/ml/brazil_all_data_v2.csv\n",
            "Resolving www.ilr.uni-bonn.de (www.ilr.uni-bonn.de)... 131.220.69.2\n",
            "Connecting to www.ilr.uni-bonn.de (www.ilr.uni-bonn.de)|131.220.69.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 789226067 (753M) [application/octet-stream]\n",
            "Saving to: ‘brazil_all_data_v2.csv’\n",
            "\n",
            "brazil_all_data_v2. 100%[===================>] 752.66M  6.06MB/s    in 3m 16s  \n",
            "\n",
            "2020-06-27 05:34:59 (3.85 MB/s) - ‘brazil_all_data_v2.csv’ saved [789226067/789226067]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfWHYVAT8Ylk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data with pandas into a dataframe \n",
        "df = pd.read_csv('brazil_all_data_v2.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WUBlF368Ylr",
        "colab_type": "text"
      },
      "source": [
        "#### Setup dependent and explantory variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivZFEoz28Yls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define binary variable for deforestration called D_defor_2018 from defor_2018\n",
        "df['D_defor_2018'] = df['defor_2018']>0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWl0aefa8Ylx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add a variable, called constant, with only ones to the dataframe\n",
        "df['constant'] = 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8gzlvci8Yl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "7241045a-bfef-48b3-d098-3f25166ae994"
      },
      "source": [
        "# View first 5 rows of the data\n",
        "df.head(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>row</th>\n",
              "      <th>col</th>\n",
              "      <th>lon</th>\n",
              "      <th>lat</th>\n",
              "      <th>bean</th>\n",
              "      <th>carrot</th>\n",
              "      <th>cassava</th>\n",
              "      <th>chickpea</th>\n",
              "      <th>citrus</th>\n",
              "      <th>coffee</th>\n",
              "      <th>groundnut</th>\n",
              "      <th>maize</th>\n",
              "      <th>soy</th>\n",
              "      <th>sugarcane</th>\n",
              "      <th>tomato</th>\n",
              "      <th>wheat</th>\n",
              "      <th>perc_treecover</th>\n",
              "      <th>perm_water</th>\n",
              "      <th>travel_min</th>\n",
              "      <th>defor_2001</th>\n",
              "      <th>defor_2002</th>\n",
              "      <th>defor_2003</th>\n",
              "      <th>defor_2004</th>\n",
              "      <th>defor_2005</th>\n",
              "      <th>defor_2006</th>\n",
              "      <th>defor_2007</th>\n",
              "      <th>defor_2008</th>\n",
              "      <th>defor_2009</th>\n",
              "      <th>defor_2010</th>\n",
              "      <th>defor_2011</th>\n",
              "      <th>defor_2012</th>\n",
              "      <th>defor_2013</th>\n",
              "      <th>defor_2014</th>\n",
              "      <th>defor_2015</th>\n",
              "      <th>defor_2016</th>\n",
              "      <th>defor_2017</th>\n",
              "      <th>defor_2018</th>\n",
              "      <th>wdpa_1990</th>\n",
              "      <th>wdpa_1991</th>\n",
              "      <th>...</th>\n",
              "      <th>tot_defor_2018_lag_1st_order</th>\n",
              "      <th>tot_defor_2001_lag_2nd_order</th>\n",
              "      <th>tot_defor_2002_lag_2nd_order</th>\n",
              "      <th>tot_defor_2003_lag_2nd_order</th>\n",
              "      <th>tot_defor_2004_lag_2nd_order</th>\n",
              "      <th>tot_defor_2005_lag_2nd_order</th>\n",
              "      <th>tot_defor_2006_lag_2nd_order</th>\n",
              "      <th>tot_defor_2007_lag_2nd_order</th>\n",
              "      <th>tot_defor_2008_lag_2nd_order</th>\n",
              "      <th>tot_defor_2009_lag_2nd_order</th>\n",
              "      <th>tot_defor_2010_lag_2nd_order</th>\n",
              "      <th>tot_defor_2011_lag_2nd_order</th>\n",
              "      <th>tot_defor_2012_lag_2nd_order</th>\n",
              "      <th>tot_defor_2013_lag_2nd_order</th>\n",
              "      <th>tot_defor_2014_lag_2nd_order</th>\n",
              "      <th>tot_defor_2015_lag_2nd_order</th>\n",
              "      <th>tot_defor_2016_lag_2nd_order</th>\n",
              "      <th>tot_defor_2017_lag_2nd_order</th>\n",
              "      <th>tot_defor_2018_lag_2nd_order</th>\n",
              "      <th>tot_defor_2001_lag_3rd_order</th>\n",
              "      <th>tot_defor_2002_lag_3rd_order</th>\n",
              "      <th>tot_defor_2003_lag_3rd_order</th>\n",
              "      <th>tot_defor_2004_lag_3rd_order</th>\n",
              "      <th>tot_defor_2005_lag_3rd_order</th>\n",
              "      <th>tot_defor_2006_lag_3rd_order</th>\n",
              "      <th>tot_defor_2007_lag_3rd_order</th>\n",
              "      <th>tot_defor_2008_lag_3rd_order</th>\n",
              "      <th>tot_defor_2009_lag_3rd_order</th>\n",
              "      <th>tot_defor_2010_lag_3rd_order</th>\n",
              "      <th>tot_defor_2011_lag_3rd_order</th>\n",
              "      <th>tot_defor_2012_lag_3rd_order</th>\n",
              "      <th>tot_defor_2013_lag_3rd_order</th>\n",
              "      <th>tot_defor_2014_lag_3rd_order</th>\n",
              "      <th>tot_defor_2015_lag_3rd_order</th>\n",
              "      <th>tot_defor_2016_lag_3rd_order</th>\n",
              "      <th>tot_defor_2017_lag_3rd_order</th>\n",
              "      <th>tot_defor_2018_lag_3rd_order</th>\n",
              "      <th>s</th>\n",
              "      <th>constant</th>\n",
              "      <th>D_defor_2018</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-59.989876</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>200.00000</td>\n",
              "      <td>335.00000</td>\n",
              "      <td>201.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>237.00000</td>\n",
              "      <td>115.0</td>\n",
              "      <td>461.00000</td>\n",
              "      <td>209.00000</td>\n",
              "      <td>1295.0000</td>\n",
              "      <td>357.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.761093</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2612.6440</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009531</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000625</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009531</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.625000</td>\n",
              "      <td>26.499998</td>\n",
              "      <td>17.500000</td>\n",
              "      <td>2.625000</td>\n",
              "      <td>2.125000</td>\n",
              "      <td>37.375000</td>\n",
              "      <td>1.125000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>11.625000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>4.875000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>9.875000</td>\n",
              "      <td>1.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.800000</td>\n",
              "      <td>14.333333</td>\n",
              "      <td>9.800000</td>\n",
              "      <td>1.533333</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1.800000</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>6.866667</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>2.200000</td>\n",
              "      <td>4.466667</td>\n",
              "      <td>9.866667</td>\n",
              "      <td>6.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-59.969875</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>200.00000</td>\n",
              "      <td>335.00000</td>\n",
              "      <td>201.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>237.00000</td>\n",
              "      <td>115.0</td>\n",
              "      <td>461.00000</td>\n",
              "      <td>209.00000</td>\n",
              "      <td>1295.0000</td>\n",
              "      <td>357.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.777657</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2680.3191</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013125</td>\n",
              "      <td>0.008437</td>\n",
              "      <td>0.011875</td>\n",
              "      <td>0.003125</td>\n",
              "      <td>0.000781</td>\n",
              "      <td>0.046719</td>\n",
              "      <td>0.001094</td>\n",
              "      <td>0.002812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002188</td>\n",
              "      <td>0.000625</td>\n",
              "      <td>0.002344</td>\n",
              "      <td>0.006094</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.006562</td>\n",
              "      <td>0.001406</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>19.999998</td>\n",
              "      <td>5.818181</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>1.363636</td>\n",
              "      <td>1.545454</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>1.818182</td>\n",
              "      <td>7.909091</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1.636364</td>\n",
              "      <td>1.272727</td>\n",
              "      <td>12.909090</td>\n",
              "      <td>10.181818</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.157895</td>\n",
              "      <td>12.052631</td>\n",
              "      <td>3.842105</td>\n",
              "      <td>0.157895</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.052631</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.105263</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.473684</td>\n",
              "      <td>9.473684</td>\n",
              "      <td>6.210527</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-59.949875</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>200.00000</td>\n",
              "      <td>335.00000</td>\n",
              "      <td>201.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>237.00000</td>\n",
              "      <td>115.0</td>\n",
              "      <td>461.00000</td>\n",
              "      <td>209.00000</td>\n",
              "      <td>1295.0000</td>\n",
              "      <td>357.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.766403</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2796.3284</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.024531</td>\n",
              "      <td>0.009375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001719</td>\n",
              "      <td>0.005313</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.173913</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.214286</td>\n",
              "      <td>8.785713</td>\n",
              "      <td>5.857143</td>\n",
              "      <td>1.642857</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>22.571428</td>\n",
              "      <td>1.785714</td>\n",
              "      <td>2.214286</td>\n",
              "      <td>2.571429</td>\n",
              "      <td>7.142858</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.357143</td>\n",
              "      <td>3.785714</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>8.571429</td>\n",
              "      <td>3.142857</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>1.826087</td>\n",
              "      <td>6.869565</td>\n",
              "      <td>7.086957</td>\n",
              "      <td>8.260869</td>\n",
              "      <td>1.782609</td>\n",
              "      <td>4.347826</td>\n",
              "      <td>18.043478</td>\n",
              "      <td>1.956522</td>\n",
              "      <td>3.652174</td>\n",
              "      <td>1.652174</td>\n",
              "      <td>5.913043</td>\n",
              "      <td>4.086957</td>\n",
              "      <td>4.521739</td>\n",
              "      <td>4.956522</td>\n",
              "      <td>8.695652</td>\n",
              "      <td>11.217392</td>\n",
              "      <td>5.173913</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-59.929874</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>200.00000</td>\n",
              "      <td>335.00000</td>\n",
              "      <td>201.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>237.00000</td>\n",
              "      <td>115.0</td>\n",
              "      <td>461.00000</td>\n",
              "      <td>209.00000</td>\n",
              "      <td>1295.0000</td>\n",
              "      <td>357.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.814842</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2920.0164</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002188</td>\n",
              "      <td>0.017812</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6.518518</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>2.928571</td>\n",
              "      <td>11.285714</td>\n",
              "      <td>18.214285</td>\n",
              "      <td>17.214285</td>\n",
              "      <td>2.928571</td>\n",
              "      <td>6.142857</td>\n",
              "      <td>28.428572</td>\n",
              "      <td>3.142857</td>\n",
              "      <td>5.357143</td>\n",
              "      <td>2.785714</td>\n",
              "      <td>8.428572</td>\n",
              "      <td>5.857142</td>\n",
              "      <td>7.214285</td>\n",
              "      <td>6.142857</td>\n",
              "      <td>6.785714</td>\n",
              "      <td>15.785715</td>\n",
              "      <td>8.214286</td>\n",
              "      <td>1.222222</td>\n",
              "      <td>2.777778</td>\n",
              "      <td>5.925926</td>\n",
              "      <td>12.185184</td>\n",
              "      <td>10.740741</td>\n",
              "      <td>2.074074</td>\n",
              "      <td>4.370370</td>\n",
              "      <td>15.555556</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>3.814815</td>\n",
              "      <td>2.666667</td>\n",
              "      <td>5.407407</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.925926</td>\n",
              "      <td>3.703704</td>\n",
              "      <td>5.888889</td>\n",
              "      <td>19.629629</td>\n",
              "      <td>6.518518</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-59.909874</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>218.33334</td>\n",
              "      <td>435.83334</td>\n",
              "      <td>216.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>523.5</td>\n",
              "      <td>317.83331</td>\n",
              "      <td>117.5</td>\n",
              "      <td>522.66663</td>\n",
              "      <td>233.16666</td>\n",
              "      <td>1300.8334</td>\n",
              "      <td>482.83331</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.655937</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2977.4216</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005469</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.222222</td>\n",
              "      <td>2.357143</td>\n",
              "      <td>5.285714</td>\n",
              "      <td>5.285714</td>\n",
              "      <td>14.785714</td>\n",
              "      <td>14.357142</td>\n",
              "      <td>2.571429</td>\n",
              "      <td>6.500000</td>\n",
              "      <td>7.571428</td>\n",
              "      <td>2.642857</td>\n",
              "      <td>5.285714</td>\n",
              "      <td>3.857143</td>\n",
              "      <td>3.642857</td>\n",
              "      <td>6.142857</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.357143</td>\n",
              "      <td>15.285714</td>\n",
              "      <td>29.714287</td>\n",
              "      <td>9.142857</td>\n",
              "      <td>10.185184</td>\n",
              "      <td>3.111111</td>\n",
              "      <td>5.925926</td>\n",
              "      <td>9.777778</td>\n",
              "      <td>10.740741</td>\n",
              "      <td>2.148148</td>\n",
              "      <td>4.111111</td>\n",
              "      <td>15.000001</td>\n",
              "      <td>1.851852</td>\n",
              "      <td>8.296296</td>\n",
              "      <td>2.629630</td>\n",
              "      <td>5.222222</td>\n",
              "      <td>7.592592</td>\n",
              "      <td>5.370370</td>\n",
              "      <td>4.481482</td>\n",
              "      <td>8.888889</td>\n",
              "      <td>18.888889</td>\n",
              "      <td>5.222222</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 428 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  row  col  ...  s  constant  D_defor_2018\n",
              "0   0    0    0  ...  1         1         False\n",
              "1   1    0    1  ...  1         1          True\n",
              "2   2    0    2  ...  1         1         False\n",
              "3   3    0    3  ...  1         1         False\n",
              "4   4    0    4  ...  1         1          True\n",
              "\n",
              "[5 rows x 428 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WU_yntf8Yl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the dependent variable\n",
        "Y = df['D_defor_2018']\n",
        "# Define a list of variable names for explanatory variables\n",
        "lstCols = [\n",
        "  'wdpa_2017',\n",
        "  'population_2015',\n",
        "  'chirps_2017',\n",
        "  'defor_2017',\n",
        "  'maize',\n",
        "  'soy',\n",
        "  'sugarcane',\n",
        "  'perc_treecover',\n",
        "  'perm_water',\n",
        "  'travel_min',\n",
        "  'cropland',\n",
        "  # 'pasture',\n",
        "  'mean_elev',\n",
        "  'sd_elev',\n",
        "  'near_road',\n",
        "  'defor_2017_lag_1st_order',\n",
        "  'wdpa_2017_lag_1st_order',\n",
        "  'chirps_2017_lag_1st_order',\n",
        "  'population_2015_lag_1st_order',\n",
        "  'maize_lag_1st_order',\n",
        "  'soy_lag_1st_order',\n",
        "  'sugarcane_lag_1st_order',\n",
        "  'perc_treecover_lag_1st_order',\n",
        "  'perm_water_lag_1st_order',\n",
        "  'travel_min_lag_1st_order',\n",
        "  'cropland_lag_1st_order',\n",
        "  # 'pasture_lag_1st_order',\n",
        "  'mean_elev_lag_1st_order',\n",
        "  'sd_elev_lag_1st_order',\n",
        "  'near_road_lag_1st_order',\n",
        "#  'bean',\n",
        "#  'carrot',\n",
        "#  'cassava',\n",
        "#  'chickpea',\n",
        "#  'citrus',\n",
        "#  'coffee',\n",
        "#  'groundnut',\n",
        "#  'maize',\n",
        "#  'soy',\n",
        "#  'sugarcane',\n",
        "#  'tomato',\n",
        "#  'wheat',\n",
        "#  'defor_2001',\n",
        "#  'defor_2002',\n",
        "#  'defor_2003',\n",
        "#  'defor_2004',\n",
        "#  'defor_2005',\n",
        "#  'defor_2006',\n",
        "#  'defor_2007',\n",
        "#  'defor_2008',\n",
        "#  'defor_2009',\n",
        "#  'defor_2010',\n",
        "#  'defor_2011',\n",
        "#  'defor_2012',\n",
        "#  'defor_2013',\n",
        "#  'defor_2014',\n",
        "#  'defor_2015',\n",
        "#  'defor_2016',\n",
        "#  'defor_2017',\n",
        "#  'near_dist_km',\n",
        "#  'mean_elev_mts',\n",
        "#  'sd_elev_mts',\n",
        " ]\n",
        "\n",
        "# Get the explanatory Variables\n",
        "X =  df.loc[:,lstCols]\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgPj9lkr8Yl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into train and test data using sklearn train_test_split object\n",
        "#   (see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "\n",

        "#   Note: This randomly splits the data in 80% train and 20% test data\n",
        "X_train_raw, X_test_raw, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"

      ],
      "execution_count": 12,
      "outputs": []
    },
     {
      "cell_type": "code",
      "metadata": {
        "id": "SVlC9obC8ggx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "388a7c8d-61e5-4ef5-91e1-c26120554b8b"
      },
      "source": [
        "X_train_raw.head(5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>wdpa_2017</th>\n",
              "      <th>population_2015</th>\n",
              "      <th>chirps_2017</th>\n",
              "      <th>defor_2017</th>\n",
              "      <th>maize</th>\n",
              "      <th>soy</th>\n",
              "      <th>sugarcane</th>\n",
              "      <th>perc_treecover</th>\n",
              "      <th>perm_water</th>\n",
              "      <th>travel_min</th>\n",
              "      <th>cropland</th>\n",
              "      <th>mean_elev</th>\n",
              "      <th>sd_elev</th>\n",
              "      <th>near_road</th>\n",
              "      <th>defor_2017_lag_1st_order</th>\n",
              "      <th>wdpa_2017_lag_1st_order</th>\n",
              "      <th>chirps_2017_lag_1st_order</th>\n",
              "      <th>population_2015_lag_1st_order</th>\n",
              "      <th>maize_lag_1st_order</th>\n",
              "      <th>soy_lag_1st_order</th>\n",
              "      <th>sugarcane_lag_1st_order</th>\n",
              "      <th>perc_treecover_lag_1st_order</th>\n",
              "      <th>perm_water_lag_1st_order</th>\n",
              "      <th>travel_min_lag_1st_order</th>\n",
              "      <th>cropland_lag_1st_order</th>\n",
              "      <th>mean_elev_lag_1st_order</th>\n",
              "      <th>sd_elev_lag_1st_order</th>\n",
              "      <th>near_road_lag_1st_order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>211019</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.743300</td>\n",
              "      <td>1140.8701</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>5145.0000</td>\n",
              "      <td>1725.00000</td>\n",
              "      <td>3498.0</td>\n",
              "      <td>95.500000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>386.034120</td>\n",
              "      <td>0.105000</td>\n",
              "      <td>233.27431</td>\n",
              "      <td>7.357755</td>\n",
              "      <td>3.982380</td>\n",
              "      <td>0.002949</td>\n",
              "      <td>0.033602</td>\n",
              "      <td>1153.6166</td>\n",
              "      <td>1.734069</td>\n",
              "      <td>4604.9746</td>\n",
              "      <td>1573.41600</td>\n",
              "      <td>3080.0693</td>\n",
              "      <td>88.958664</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475.483760</td>\n",
              "      <td>0.074437</td>\n",
              "      <td>251.05762</td>\n",
              "      <td>12.067036</td>\n",
              "      <td>4.300830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152100</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.497108</td>\n",
              "      <td>1494.5341</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1418.8333</td>\n",
              "      <td>566.83337</td>\n",
              "      <td>2012.0</td>\n",
              "      <td>0.321094</td>\n",
              "      <td>1.0</td>\n",
              "      <td>52.908264</td>\n",
              "      <td>0.019833</td>\n",
              "      <td>127.98780</td>\n",
              "      <td>1.093633</td>\n",
              "      <td>3.355971</td>\n",
              "      <td>0.002728</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1499.3014</td>\n",
              "      <td>0.540844</td>\n",
              "      <td>1446.2916</td>\n",
              "      <td>584.61456</td>\n",
              "      <td>2058.0938</td>\n",
              "      <td>17.326862</td>\n",
              "      <td>1.002894</td>\n",
              "      <td>71.333534</td>\n",
              "      <td>0.023635</td>\n",
              "      <td>128.69191</td>\n",
              "      <td>2.285338</td>\n",
              "      <td>2.511065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71133</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.120582</td>\n",
              "      <td>1922.1539</td>\n",
              "      <td>0.004062</td>\n",
              "      <td>1189.6666</td>\n",
              "      <td>700.00000</td>\n",
              "      <td>2763.0</td>\n",
              "      <td>90.612343</td>\n",
              "      <td>1.0</td>\n",
              "      <td>491.777890</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>323.18399</td>\n",
              "      <td>7.377765</td>\n",
              "      <td>64.657524</td>\n",
              "      <td>0.004320</td>\n",
              "      <td>0.014956</td>\n",
              "      <td>1928.2488</td>\n",
              "      <td>0.095600</td>\n",
              "      <td>1189.5186</td>\n",
              "      <td>695.65045</td>\n",
              "      <td>2767.7256</td>\n",
              "      <td>89.219002</td>\n",
              "      <td>1.002949</td>\n",
              "      <td>639.929750</td>\n",
              "      <td>0.006477</td>\n",
              "      <td>316.10376</td>\n",
              "      <td>9.349813</td>\n",
              "      <td>64.806252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113252</th>\n",
              "      <td>0.938755</td>\n",
              "      <td>0.083385</td>\n",
              "      <td>1837.0503</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1189.0000</td>\n",
              "      <td>760.00000</td>\n",
              "      <td>2957.0</td>\n",
              "      <td>5.153281</td>\n",
              "      <td>1.0</td>\n",
              "      <td>218.281520</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>471.76910</td>\n",
              "      <td>14.875660</td>\n",
              "      <td>2.990992</td>\n",
              "      <td>0.002210</td>\n",
              "      <td>0.500785</td>\n",
              "      <td>1850.5565</td>\n",
              "      <td>0.086631</td>\n",
              "      <td>1160.1609</td>\n",
              "      <td>743.83160</td>\n",
              "      <td>2938.7192</td>\n",
              "      <td>13.188030</td>\n",
              "      <td>1.000645</td>\n",
              "      <td>238.731700</td>\n",
              "      <td>0.011638</td>\n",
              "      <td>446.91449</td>\n",
              "      <td>9.372373</td>\n",
              "      <td>2.360994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32717</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.340720</td>\n",
              "      <td>2036.5881</td>\n",
              "      <td>0.002656</td>\n",
              "      <td>1170.0000</td>\n",
              "      <td>631.00000</td>\n",
              "      <td>2845.0</td>\n",
              "      <td>92.805466</td>\n",
              "      <td>1.0</td>\n",
              "      <td>704.614990</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>336.11801</td>\n",
              "      <td>9.273241</td>\n",
              "      <td>197.730700</td>\n",
              "      <td>0.027669</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2063.5518</td>\n",
              "      <td>0.690254</td>\n",
              "      <td>1148.6683</td>\n",
              "      <td>626.19678</td>\n",
              "      <td>2789.2783</td>\n",
              "      <td>77.084564</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>693.703980</td>\n",
              "      <td>0.002519</td>\n",
              "      <td>352.24255</td>\n",
              "      <td>10.586811</td>\n",
              "      <td>197.779630</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        wdpa_2017  ...  near_road_lag_1st_order\n",
              "211019   0.000000  ...                 4.300830\n",
              "152100   0.000000  ...                 2.511065\n",
              "71133    0.000000  ...                64.806252\n",
              "113252   0.938755  ...                 2.360994\n",
              "32717    0.000000  ...               197.779630\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },  
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFhM3lFp8YmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale data to 0-1 range using sklearn MinMaxScalar object. This facilitates training the model \n",
        "# (see: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) \n",
        "scaler = MinMaxScaler()\n",
        "# Use only the train data to fit the MinMaxScalar \n",
        "scaler.fit(X_train_raw)\n",
        "\n",
        "# Apply the MinMax transformation to the train and test data \n",
        "X_train = scaler.transform(X_train_raw)\n",
        "X_test = scaler.transform(X_test_raw)\n",
        "# Note the depended variable does not need to be scaled as it is a binary variable anyway"
      ],
      "execution_count": 13,
      "outputs": []
    },
 {
      "cell_type": "code",
      "metadata": {
        "id": "MCc0Axm79fvz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "e6f1415a-103a-4039-dfbe-8c9b4abaaae9"
      },
      "source": [
        "traindf = pd.DataFrame(X_train)\n",
        "traindf.head(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>0.162163</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.813011</td>\n",
              "      <td>0.850299</td>\n",
              "      <td>0.555788</td>\n",
              "      <td>0.955000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.101330</td>\n",
              "      <td>0.105000</td>\n",
              "      <td>0.163486</td>\n",
              "      <td>0.035448</td>\n",
              "      <td>0.011118</td>\n",
              "      <td>0.006585</td>\n",
              "      <td>0.033602</td>\n",
              "      <td>0.169992</td>\n",
              "      <td>0.000790</td>\n",
              "      <td>0.740660</td>\n",
              "      <td>0.773393</td>\n",
              "      <td>0.466654</td>\n",
              "      <td>0.889646</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.130121</td>\n",
              "      <td>0.077217</td>\n",
              "      <td>0.188011</td>\n",
              "      <td>0.122987</td>\n",
              "      <td>0.009348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.353761</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.215964</td>\n",
              "      <td>0.272372</td>\n",
              "      <td>0.316188</td>\n",
              "      <td>0.003211</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013660</td>\n",
              "      <td>0.019833</td>\n",
              "      <td>0.052599</td>\n",
              "      <td>0.005269</td>\n",
              "      <td>0.009369</td>\n",
              "      <td>0.006091</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.362836</td>\n",
              "      <td>0.000246</td>\n",
              "      <td>0.205607</td>\n",
              "      <td>0.249477</td>\n",
              "      <td>0.292944</td>\n",
              "      <td>0.173261</td>\n",
              "      <td>0.003568</td>\n",
              "      <td>0.017938</td>\n",
              "      <td>0.024518</td>\n",
              "      <td>0.053661</td>\n",
              "      <td>0.019159</td>\n",
              "      <td>0.004279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.585425</td>\n",
              "      <td>0.004071</td>\n",
              "      <td>0.179245</td>\n",
              "      <td>0.338822</td>\n",
              "      <td>0.437278</td>\n",
              "      <td>0.906123</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.129159</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>0.258179</td>\n",
              "      <td>0.035544</td>\n",
              "      <td>0.180510</td>\n",
              "      <td>0.009646</td>\n",
              "      <td>0.014956</td>\n",
              "      <td>0.602130</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.162112</td>\n",
              "      <td>0.308310</td>\n",
              "      <td>0.413564</td>\n",
              "      <td>0.892249</td>\n",
              "      <td>0.003636</td>\n",
              "      <td>0.175768</td>\n",
              "      <td>0.006719</td>\n",
              "      <td>0.259428</td>\n",
              "      <td>0.094145</td>\n",
              "      <td>0.180721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.938755</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.539320</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.179138</td>\n",
              "      <td>0.368762</td>\n",
              "      <td>0.468559</td>\n",
              "      <td>0.051533</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.057182</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.414668</td>\n",
              "      <td>0.071667</td>\n",
              "      <td>0.008350</td>\n",
              "      <td>0.004935</td>\n",
              "      <td>0.500785</td>\n",
              "      <td>0.558788</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.157139</td>\n",
              "      <td>0.333838</td>\n",
              "      <td>0.442628</td>\n",
              "      <td>0.131869</td>\n",
              "      <td>0.000795</td>\n",
              "      <td>0.064404</td>\n",
              "      <td>0.012072</td>\n",
              "      <td>0.403050</td>\n",
              "      <td>0.094385</td>\n",
              "      <td>0.003853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.647420</td>\n",
              "      <td>0.002662</td>\n",
              "      <td>0.176094</td>\n",
              "      <td>0.304391</td>\n",
              "      <td>0.450500</td>\n",
              "      <td>0.928055</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.185172</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.271801</td>\n",
              "      <td>0.044676</td>\n",
              "      <td>0.552020</td>\n",
              "      <td>0.061784</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.677610</td>\n",
              "      <td>0.000314</td>\n",
              "      <td>0.155192</td>\n",
              "      <td>0.271510</td>\n",
              "      <td>0.417227</td>\n",
              "      <td>0.770894</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.190694</td>\n",
              "      <td>0.002613</td>\n",
              "      <td>0.299106</td>\n",
              "      <td>0.107275</td>\n",
              "      <td>0.557350</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        25        26        27\n",
              "0  0.000000  0.000264  0.162163  ...  0.188011  0.122987  0.009348\n",
              "1  0.000000  0.000075  0.353761  ...  0.053661  0.019159  0.004279\n",
              "2  0.000000  0.000018  0.585425  ...  0.259428  0.094145  0.180721\n",
              "3  0.938755  0.000013  0.539320  ...  0.403050  0.094385  0.003853\n",
              "4  0.000000  0.000052  0.647420  ...  0.299106  0.107275  0.557350\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F45G02gc8YmH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "928a4a57-5e57-4815-f9e7-2e46c2b4fc8c"
      },
      "source": [
        "# Fit a logistic regression model using sklearn (see: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "# Create the model object\n",
        "modelLg = LogisticRegression(random_state=0,penalty='none',fit_intercept=True, max_iter=1000)\n",
        "# Fit the model using the training data\n",
        "modelLg.fit(X_train, Y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
              "                   multi_class='auto', n_jobs=None, penalty='none',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-yYdgmJ8YmM",
        "colab_type": "text"
      },
      "source": [
        "### Note: \n",
        "sklearn is a popular ML libary that we will primarily use in the course. While sklearn allows to run\n",
        "regressions it does not provide regression table outputs (with p-values, standard errors etc.). \n",
        "While those table are very common in econometrics they are not commonly considered in the ML \n",
        "community. For illustrative puposes we do the calculation for a regrssion table manually, however,\n",
        "there is also a \"statsmodels\" libary in python that does this automatically (see below). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWVvG9Nb8YmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate pvalues and standard errors for a scikit-learn logisticRegression\n",
        "# Source: https://stackoverflow.com/questions/25122999/scikit-learn-how-to-check-coefficients-significance\n",
        "def logit_pvalue(model, x):\n",
        "    \"\"\" Calculate z-scores for scikit-learn LogisticRegression.\n",
        "    parameters:\n",
        "        model: fitted sklearn.linear_model.LogisticRegression with intercept and large C\n",
        "        x:     matrix on which the model was fit\n",
        "    This function uses asymtptics for maximum likelihood estimates.\n",
        "    \"\"\"\n",
        "    p = model.predict_proba(x)\n",
        "    n = len(p)\n",
        "    m = len(model.coef_[0]) + 1\n",
        "    # m = len(model.coef_[0])\n",
        "    # coefs = model.coef_[0]\n",
        "    coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
        "    x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
        "    ans = np.zeros((m, m))\n",
        "    for i in range(n):\n",
        "        ans = ans + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p[i,1] * p[i, 0]\n",
        "    vcov = np.linalg.inv(np.matrix(ans))\n",
        "    se = np.sqrt(np.diag(vcov))\n",
        "    t =  coefs/se  \n",
        "    p = (1 - norm.cdf(abs(t))) * 2\n",
        "    return se, p"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ORdUGhf8YmR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "outputId": "59af3e21-a4e5-4d0e-d6a5-cc3d38dba105"
      },
      "source": [
        "# Use the previously created function to create a regression output table\n",
        "se, p = logit_pvalue(modelLg, X_train)\n",
        "coefs = np.concatenate([modelLg.intercept_, modelLg.coef_[0]]).T\n",
        "resCoef = pd.DataFrame(coefs,index=['constant']+lstCols)\n",
        "resCoef.columns = ['coef']\n",
        "resCoef['se'] = se\n",
        "resCoef['pval'] = p\n",
        "resCoef"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coef</th>\n",
              "      <th>se</th>\n",
              "      <th>pval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>constant</th>\n",
              "      <td>-1.898514</td>\n",
              "      <td>0.034348</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wdpa_2017</th>\n",
              "      <td>-0.453089</td>\n",
              "      <td>0.064172</td>\n",
              "      <td>1.658895e-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>population_2015</th>\n",
              "      <td>-0.478958</td>\n",
              "      <td>0.588701</td>\n",
              "      <td>4.158829e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chirps_2017</th>\n",
              "      <td>-1.223988</td>\n",
              "      <td>0.689961</td>\n",
              "      <td>7.606378e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>defor_2017</th>\n",
              "      <td>13.976782</td>\n",
              "      <td>0.284723</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maize</th>\n",
              "      <td>2.393496</td>\n",
              "      <td>0.771121</td>\n",
              "      <td>1.909752e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>soy</th>\n",
              "      <td>-0.973235</td>\n",
              "      <td>0.576856</td>\n",
              "      <td>9.157727e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sugarcane</th>\n",
              "      <td>0.110815</td>\n",
              "      <td>0.509138</td>\n",
              "      <td>8.277005e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perc_treecover</th>\n",
              "      <td>0.652472</td>\n",
              "      <td>0.032820</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perm_water</th>\n",
              "      <td>0.717640</td>\n",
              "      <td>0.262387</td>\n",
              "      <td>6.237201e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>travel_min</th>\n",
              "      <td>-0.785305</td>\n",
              "      <td>0.343719</td>\n",
              "      <td>2.232857e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cropland</th>\n",
              "      <td>-0.129292</td>\n",
              "      <td>0.187495</td>\n",
              "      <td>4.904637e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean_elev</th>\n",
              "      <td>-4.288318</td>\n",
              "      <td>0.203019</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sd_elev</th>\n",
              "      <td>0.522149</td>\n",
              "      <td>0.135124</td>\n",
              "      <td>1.114417e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>near_road</th>\n",
              "      <td>-6.151552</td>\n",
              "      <td>1.787758</td>\n",
              "      <td>5.797159e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>defor_2017_lag_1st_order</th>\n",
              "      <td>3.667407</td>\n",
              "      <td>0.163751</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wdpa_2017_lag_1st_order</th>\n",
              "      <td>-0.926047</td>\n",
              "      <td>0.071323</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chirps_2017_lag_1st_order</th>\n",
              "      <td>4.063151</td>\n",
              "      <td>0.678606</td>\n",
              "      <td>2.130927e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>population_2015_lag_1st_order</th>\n",
              "      <td>3.394481</td>\n",
              "      <td>0.407055</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maize_lag_1st_order</th>\n",
              "      <td>-2.562684</td>\n",
              "      <td>0.752039</td>\n",
              "      <td>6.552560e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>soy_lag_1st_order</th>\n",
              "      <td>2.982555</td>\n",
              "      <td>0.557923</td>\n",
              "      <td>9.000987e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sugarcane_lag_1st_order</th>\n",
              "      <td>-4.035659</td>\n",
              "      <td>0.501431</td>\n",
              "      <td>8.881784e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perc_treecover_lag_1st_order</th>\n",
              "      <td>0.106670</td>\n",
              "      <td>0.047136</td>\n",
              "      <td>2.363389e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perm_water_lag_1st_order</th>\n",
              "      <td>-5.313571</td>\n",
              "      <td>0.452926</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>travel_min_lag_1st_order</th>\n",
              "      <td>-1.947078</td>\n",
              "      <td>0.352747</td>\n",
              "      <td>3.394567e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cropland_lag_1st_order</th>\n",
              "      <td>0.747673</td>\n",
              "      <td>0.202067</td>\n",
              "      <td>2.154905e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean_elev_lag_1st_order</th>\n",
              "      <td>2.816691</td>\n",
              "      <td>0.206213</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sd_elev_lag_1st_order</th>\n",
              "      <td>1.856931</td>\n",
              "      <td>0.099220</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>near_road_lag_1st_order</th>\n",
              "      <td>7.057747</td>\n",
              "      <td>1.766505</td>\n",
              "      <td>6.460777e-05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    coef        se          pval\n",
              "constant                       -1.898514  0.034348  0.000000e+00\n",
              "wdpa_2017                      -0.453089  0.064172  1.658895e-12\n",
              "population_2015                -0.478958  0.588701  4.158829e-01\n",
              "chirps_2017                    -1.223988  0.689961  7.606378e-02\n",
              "defor_2017                     13.976782  0.284723  0.000000e+00\n",
              "maize                           2.393496  0.771121  1.909752e-03\n",
              "soy                            -0.973235  0.576856  9.157727e-02\n",
              "sugarcane                       0.110815  0.509138  8.277005e-01\n",
              "perc_treecover                  0.652472  0.032820  0.000000e+00\n",
              "perm_water                      0.717640  0.262387  6.237201e-03\n",
              "travel_min                     -0.785305  0.343719  2.232857e-02\n",
              "cropland                       -0.129292  0.187495  4.904637e-01\n",
              "mean_elev                      -4.288318  0.203019  0.000000e+00\n",
              "sd_elev                         0.522149  0.135124  1.114417e-04\n",
              "near_road                      -6.151552  1.787758  5.797159e-04\n",
              "defor_2017_lag_1st_order        3.667407  0.163751  0.000000e+00\n",
              "wdpa_2017_lag_1st_order        -0.926047  0.071323  0.000000e+00\n",
              "chirps_2017_lag_1st_order       4.063151  0.678606  2.130927e-09\n",
              "population_2015_lag_1st_order   3.394481  0.407055  0.000000e+00\n",
              "maize_lag_1st_order            -2.562684  0.752039  6.552560e-04\n",
              "soy_lag_1st_order               2.982555  0.557923  9.000987e-08\n",
              "sugarcane_lag_1st_order        -4.035659  0.501431  8.881784e-16\n",
              "perc_treecover_lag_1st_order    0.106670  0.047136  2.363389e-02\n",
              "perm_water_lag_1st_order       -5.313571  0.452926  0.000000e+00\n",
              "travel_min_lag_1st_order       -1.947078  0.352747  3.394567e-08\n",
              "cropland_lag_1st_order          0.747673  0.202067  2.154905e-04\n",
              "mean_elev_lag_1st_order         2.816691  0.206213  0.000000e+00\n",
              "sd_elev_lag_1st_order           1.856931  0.099220  0.000000e+00\n",
              "near_road_lag_1st_order         7.057747  1.766505  6.460777e-05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRYFl12K8YmV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "71d9dad6-db25-4cd8-98f9-2ac9500fb366"
      },
      "source": [
        "# Confirm the results using statsmodels\n",
        "import statsmodels.api as sm\n",
        "# Add constant to X matrix\n",
        "X_train_const = np.matrix(np.insert(np.array(X_train), 0, 1, axis = 1))\n",
        "\n",
        "# Define the logit regression\n",
        "logit = sm.Logit(Y_train,X_train_const)\n",
        "\n",
        "# Set the names of the explanatory variables\n",
        "logit.data.xnames = exog_names=['const']+lstCols\n",
        "\n",
        "# fit the model\n",
        "result = logit.fit()\n",
        "# Print the summary table\n",
        "print(result.summary())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.469965\n",
            "         Iterations 7\n",
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:           D_defor_2018   No. Observations:               199952\n",
            "Model:                          Logit   Df Residuals:                   199923\n",
            "Method:                           MLE   Df Model:                           28\n",
            "Date:                Sat, 27 Jun 2020   Pseudo R-squ.:                  0.1455\n",
            "Time:                        05:38:15   Log-Likelihood:                -93971.\n",
            "converged:                       True   LL-Null:                   -1.0997e+05\n",
            "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
            "=================================================================================================\n",
            "                                    coef    std err          z      P>|z|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------------------\n",
            "const                            -1.8960      0.034    -55.202      0.000      -1.963      -1.829\n",
            "wdpa_2017                        -0.4523      0.064     -7.048      0.000      -0.578      -0.327\n",
            "population_2015                  -0.5244      0.590     -0.888      0.374      -1.682       0.633\n",
            "chirps_2017                      -1.1744      0.690     -1.702      0.089      -2.527       0.178\n",
            "defor_2017                       13.9752      0.285     49.088      0.000      13.417      14.533\n",
            "maize                             2.6117      0.771      3.387      0.001       1.100       4.123\n",
            "soy                              -1.0624      0.577     -1.842      0.066      -2.193       0.068\n",
            "sugarcane                         0.0521      0.509      0.102      0.919      -0.946       1.050\n",
            "perc_treecover                    0.6524      0.033     19.877      0.000       0.588       0.717\n",
            "perm_water                        0.7189      0.262      2.739      0.006       0.205       1.233\n",
            "travel_min                       -0.7733      0.344     -2.250      0.024      -1.447      -0.100\n",
            "cropland                         -0.1197      0.187     -0.639      0.523      -0.487       0.248\n",
            "mean_elev                        -4.2879      0.203    -21.119      0.000      -4.686      -3.890\n",
            "sd_elev                           0.5194      0.135      3.844      0.000       0.255       0.784\n",
            "near_road                        -6.3055      1.788     -3.527      0.000      -9.810      -2.801\n",
            "defor_2017_lag_1st_order          3.6663      0.164     22.390      0.000       3.345       3.987\n",
            "wdpa_2017_lag_1st_order          -0.9267      0.071    -12.993      0.000      -1.066      -0.787\n",
            "chirps_2017_lag_1st_order         4.0124      0.679      5.913      0.000       2.682       5.342\n",
            "population_2015_lag_1st_order     3.4154      0.407      8.385      0.000       2.617       4.214\n",
            "maize_lag_1st_order              -2.7747      0.752     -3.689      0.000      -4.249      -1.300\n",
            "soy_lag_1st_order                 3.0682      0.558      5.499      0.000       1.975       4.162\n",
            "sugarcane_lag_1st_order          -3.9777      0.501     -7.933      0.000      -4.961      -2.995\n",
            "perc_treecover_lag_1st_order      0.1072      0.047      2.274      0.023       0.015       0.200\n",
            "perm_water_lag_1st_order         -5.3199      0.453    -11.741      0.000      -6.208      -4.432\n",
            "travel_min_lag_1st_order         -1.9588      0.353     -5.553      0.000      -2.650      -1.267\n",
            "cropland_lag_1st_order            0.7377      0.202      3.650      0.000       0.342       1.134\n",
            "mean_elev_lag_1st_order           2.8161      0.206     13.655      0.000       2.412       3.220\n",
            "sd_elev_lag_1st_order             1.8567      0.099     18.712      0.000       1.662       2.051\n",
            "near_road_lag_1st_order           7.2102      1.767      4.081      0.000       3.748      10.673\n",
            "=================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwbd1OKA8Ymg",
        "colab_type": "text"
      },
      "source": [
        "## Train your first very (very) simple neural network using sklearn\n",
        "Now use a neural network for the same problem. In the course you will see that this is actually equivalent to a logistic regression, hence a logistic regression is in fact a specific form of a neural network!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_6bKms58Ymg",
        "colab_type": "text"
      },
      "source": [
        "### Perform a hyper parameter search to tune the learning rate for training the NN. \n",
        "This step is optional and takes a while. You can also run the next cell, \n",
        "using a fixed learning rate. The learning rate was determined using this hyper parameter search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwpfQ5I8Ymi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b401c830-9fb7-46bd-b8f9-ffddf837d867"
      },
      "source": [
        "from sklearn.utils.fixes import loguniform\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'alpha':loguniform(1e-6, 1e-1)}\n",
        "\n",
        "modelNN = MLPClassifier(solver='lbfgs', activation = 'identity',\n",
        "                     hidden_layer_sizes=(1), random_state=1, verbose=True,max_iter=200)\n",
        "\n",
        "\n",
        "clf = RandomizedSearchCV(modelNN, param_grid, random_state=0,n_iter=10,cv=5)\n",
        "modelNN = clf.fit(X_train_const, Y_train)\n",
        "modelNN.best_params_"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 8.264328927007723e-05}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuVenVfW8Ymk",
        "colab_type": "text"
      },
      "source": [
        "### Train the Neural Network with a fixed set of hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPdSWMNX8Yml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "77f6dc8c-d780-4f80-b9bf-264555066b52"
      },
      "source": [
        "modelNN = MLPClassifier(solver='lbfgs', alpha=8.264328927007723e-05,activation = 'identity',\n",
        "                     hidden_layer_sizes=(1), random_state=1, verbose=True,max_iter=200)\n",
        "\n",
        "modelNN.fit(X_train_const, Y_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='identity', alpha=8.264328927007723e-05,\n",
              "              batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
              "              epsilon=1e-08, hidden_layer_sizes=1, learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=1, shuffle=True, solver='lbfgs',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=True,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N3xe5ve8Ymo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "outputId": "711f69bf-94ce-43f3-96ec-743f3de4f339"
      },
      "source": [
        "# Add the estimated coefficient of the NN to the regression table we created above-\n",
        "# In the course we will discuss why the estimated coefficient are similar. \n",
        "#    modelNN.coefs_[0] are the coefficients of the first layer\n",
        "#    modelNN.coefs_[1][0][0] is the coefficients of the hidden layer\n",
        "resCoef['coef_NN'] = modelNN.coefs_[0]*modelNN.coefs_[1][0][0]\n",
        "resCoef"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coef</th>\n",
              "      <th>se</th>\n",
              "      <th>pval</th>\n",
              "      <th>coef_NN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>constant</th>\n",
              "      <td>-1.898514</td>\n",
              "      <td>0.034348</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wdpa_2017</th>\n",
              "      <td>-0.453089</td>\n",
              "      <td>0.064172</td>\n",
              "      <td>1.658895e-12</td>\n",
              "      <td>-0.461436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>population_2015</th>\n",
              "      <td>-0.478958</td>\n",
              "      <td>0.588701</td>\n",
              "      <td>4.158829e-01</td>\n",
              "      <td>0.290075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chirps_2017</th>\n",
              "      <td>-1.223988</td>\n",
              "      <td>0.689961</td>\n",
              "      <td>7.606378e-02</td>\n",
              "      <td>0.728851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>defor_2017</th>\n",
              "      <td>13.976782</td>\n",
              "      <td>0.284723</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>13.774130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maize</th>\n",
              "      <td>2.393496</td>\n",
              "      <td>0.771121</td>\n",
              "      <td>1.909752e-03</td>\n",
              "      <td>0.565301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>soy</th>\n",
              "      <td>-0.973235</td>\n",
              "      <td>0.576856</td>\n",
              "      <td>9.157727e-02</td>\n",
              "      <td>0.969879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sugarcane</th>\n",
              "      <td>0.110815</td>\n",
              "      <td>0.509138</td>\n",
              "      <td>8.277005e-01</td>\n",
              "      <td>-0.864923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perc_treecover</th>\n",
              "      <td>0.652472</td>\n",
              "      <td>0.032820</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.666131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perm_water</th>\n",
              "      <td>0.717640</td>\n",
              "      <td>0.262387</td>\n",
              "      <td>6.237201e-03</td>\n",
              "      <td>0.757252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>travel_min</th>\n",
              "      <td>-0.785305</td>\n",
              "      <td>0.343719</td>\n",
              "      <td>2.232857e-02</td>\n",
              "      <td>-1.219014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cropland</th>\n",
              "      <td>-0.129292</td>\n",
              "      <td>0.187495</td>\n",
              "      <td>4.904637e-01</td>\n",
              "      <td>-0.139107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean_elev</th>\n",
              "      <td>-4.288318</td>\n",
              "      <td>0.203019</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-4.528099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sd_elev</th>\n",
              "      <td>0.522149</td>\n",
              "      <td>0.135124</td>\n",
              "      <td>1.114417e-04</td>\n",
              "      <td>0.421329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>near_road</th>\n",
              "      <td>-6.151552</td>\n",
              "      <td>1.787758</td>\n",
              "      <td>5.797159e-04</td>\n",
              "      <td>0.230797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>defor_2017_lag_1st_order</th>\n",
              "      <td>3.667407</td>\n",
              "      <td>0.163751</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.693943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wdpa_2017_lag_1st_order</th>\n",
              "      <td>-0.926047</td>\n",
              "      <td>0.071323</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.924656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chirps_2017_lag_1st_order</th>\n",
              "      <td>4.063151</td>\n",
              "      <td>0.678606</td>\n",
              "      <td>2.130927e-09</td>\n",
              "      <td>2.187772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>population_2015_lag_1st_order</th>\n",
              "      <td>3.394481</td>\n",
              "      <td>0.407055</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.131609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maize_lag_1st_order</th>\n",
              "      <td>-2.562684</td>\n",
              "      <td>0.752039</td>\n",
              "      <td>6.552560e-04</td>\n",
              "      <td>-0.760120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>soy_lag_1st_order</th>\n",
              "      <td>2.982555</td>\n",
              "      <td>0.557923</td>\n",
              "      <td>9.000987e-08</td>\n",
              "      <td>1.114618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sugarcane_lag_1st_order</th>\n",
              "      <td>-4.035659</td>\n",
              "      <td>0.501431</td>\n",
              "      <td>8.881784e-16</td>\n",
              "      <td>-3.137264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perc_treecover_lag_1st_order</th>\n",
              "      <td>0.106670</td>\n",
              "      <td>0.047136</td>\n",
              "      <td>2.363389e-02</td>\n",
              "      <td>0.087303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perm_water_lag_1st_order</th>\n",
              "      <td>-5.313571</td>\n",
              "      <td>0.452926</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-5.065223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>travel_min_lag_1st_order</th>\n",
              "      <td>-1.947078</td>\n",
              "      <td>0.352747</td>\n",
              "      <td>3.394567e-08</td>\n",
              "      <td>-1.508767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cropland_lag_1st_order</th>\n",
              "      <td>0.747673</td>\n",
              "      <td>0.202067</td>\n",
              "      <td>2.154905e-04</td>\n",
              "      <td>0.743544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean_elev_lag_1st_order</th>\n",
              "      <td>2.816691</td>\n",
              "      <td>0.206213</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.079412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sd_elev_lag_1st_order</th>\n",
              "      <td>1.856931</td>\n",
              "      <td>0.099220</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.872977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>near_road_lag_1st_order</th>\n",
              "      <td>7.057747</td>\n",
              "      <td>1.766505</td>\n",
              "      <td>6.460777e-05</td>\n",
              "      <td>0.752489</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    coef        se          pval    coef_NN\n",
              "constant                       -1.898514  0.034348  0.000000e+00  -0.208500\n",
              "wdpa_2017                      -0.453089  0.064172  1.658895e-12  -0.461436\n",
              "population_2015                -0.478958  0.588701  4.158829e-01   0.290075\n",
              "chirps_2017                    -1.223988  0.689961  7.606378e-02   0.728851\n",
              "defor_2017                     13.976782  0.284723  0.000000e+00  13.774130\n",
              "maize                           2.393496  0.771121  1.909752e-03   0.565301\n",
              "soy                            -0.973235  0.576856  9.157727e-02   0.969879\n",
              "sugarcane                       0.110815  0.509138  8.277005e-01  -0.864923\n",
              "perc_treecover                  0.652472  0.032820  0.000000e+00   0.666131\n",
              "perm_water                      0.717640  0.262387  6.237201e-03   0.757252\n",
              "travel_min                     -0.785305  0.343719  2.232857e-02  -1.219014\n",
              "cropland                       -0.129292  0.187495  4.904637e-01  -0.139107\n",
              "mean_elev                      -4.288318  0.203019  0.000000e+00  -4.528099\n",
              "sd_elev                         0.522149  0.135124  1.114417e-04   0.421329\n",
              "near_road                      -6.151552  1.787758  5.797159e-04   0.230797\n",
              "defor_2017_lag_1st_order        3.667407  0.163751  0.000000e+00   3.693943\n",
              "wdpa_2017_lag_1st_order        -0.926047  0.071323  0.000000e+00  -0.924656\n",
              "chirps_2017_lag_1st_order       4.063151  0.678606  2.130927e-09   2.187772\n",
              "population_2015_lag_1st_order   3.394481  0.407055  0.000000e+00   3.131609\n",
              "maize_lag_1st_order            -2.562684  0.752039  6.552560e-04  -0.760120\n",
              "soy_lag_1st_order               2.982555  0.557923  9.000987e-08   1.114618\n",
              "sugarcane_lag_1st_order        -4.035659  0.501431  8.881784e-16  -3.137264\n",
              "perc_treecover_lag_1st_order    0.106670  0.047136  2.363389e-02   0.087303\n",
              "perm_water_lag_1st_order       -5.313571  0.452926  0.000000e+00  -5.065223\n",
              "travel_min_lag_1st_order       -1.947078  0.352747  3.394567e-08  -1.508767\n",
              "cropland_lag_1st_order          0.747673  0.202067  2.154905e-04   0.743544\n",
              "mean_elev_lag_1st_order         2.816691  0.206213  0.000000e+00   3.079412\n",
              "sd_elev_lag_1st_order           1.856931  0.099220  0.000000e+00   1.872977\n",
              "near_road_lag_1st_order         7.057747  1.766505  6.460777e-05   0.752489"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGP8hkW28Yms",
        "colab_type": "text"
      },
      "source": [
        "### Compare the model outcomes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT1FnBrn8Ymu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add constant to the test data\n",
        "X_test_const = np.matrix(np.insert(np.array(X_test), 0, 1, axis = 1))\n",
        "# Get predicted values from logit model \n",
        "Y_test_Lg = modelLg.predict(X_test)\n",
        "# Get predicted values from NN model \n",
        "Y_test_NN = modelNN.predict(X_test_const)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um1zNIHj8Ymx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "278c037a-77fc-4f30-dca2-90d330b7c230"
      },
      "source": [
        "score_Lg = np.sum(Y_test==Y_test_Lg)/Y_test.shape[0]\n",
        "score_NN = np.sum(Y_test==Y_test_NN)/Y_test.shape[0]\n",
        "print('Score lg (R²): ',score_Lg)\n",
        "print('Score NN (R²): ',score_NN)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score lg (R²):  0.7862887092902296\n",
            "Score NN (R²):  0.7863487236936865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKKjn2W58Ym2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "ed7b5b55-9c27-4371-9320-406449710605"
      },
      "source": [
        "# plot the predicte probabalities of the logit model\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "\n",
        "pd.DataFrame(modelLg.predict_proba(X_test))[1].hist(bins=100,ax=ax1)\n",
        "pd.DataFrame(modelNN.predict_proba(X_test_const))[1].hist(bins=100,ax=ax2)\n",
        "fig.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DerYBK448Ym5",
        "colab_type": "text"
      },
      "source": [
        "### Well done!!! \n",
        "Now it is your turn. Play around with the notebook to make your very first steps with numpy/pandas and sklearn. In the intro text in the beginning there are some suggestions of what you can try.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTeZV4aa8Ym6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
